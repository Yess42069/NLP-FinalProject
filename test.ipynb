{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaForSequenceClassification\n",
    "from transformers import AdamW\n",
    "import torch\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import os\n",
    "from d2l import torch as d2l\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"data/aclImdb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@save\n",
    "def read_imdb(data_dir, is_train):\n",
    "    \"\"\"Read the IMDb review dataset text sequences and labels.\"\"\"\n",
    "    ### YOUR CODE HERE\n",
    "    data = []\n",
    "    labels = []\n",
    "    \n",
    "    dir = 'train' if is_train else 'test'\n",
    "    \n",
    "    for label in ['pos', 'neg']:\n",
    "        # Directory path for each postive and negative\n",
    "        dir_path = os.path.join(data_dir, dir, label)\n",
    "        for file_name in os.listdir(dir_path):\n",
    "            file_path = os.path.join(dir_path, file_name)\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                review = f.read()\n",
    "                data.append(review)\n",
    "                # Assign label = 1 for positive and 0 for negative\n",
    "                labels.append(1 if label == 'pos' else 0)\n",
    "    ### END OF YOUR CODE\n",
    "    return data, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@save\n",
    "def load_data_imdb(batch_size, num_steps=500):\n",
    "    \"\"\"Return data iterators and the vocabulary of the IMDb review dataset.\"\"\"\n",
    "    ### YOUR CODE HERE\n",
    "    train_data = read_imdb(data_dir, is_train=True)\n",
    "    test_data = read_imdb(data_dir, is_train=False)\n",
    "\n",
    "    train_tokens = d2l.tokenize(train_data[0], token='word')\n",
    "    train_vocab = d2l.Vocab(train_tokens, min_freq=5, reserved_tokens=['<pad>'])\n",
    "\n",
    "    test_tokens = d2l.tokenize(test_data[0], token='word')\n",
    "    test_vocab = d2l.Vocab(test_tokens, min_freq=5, reserved_tokens=['<pad>'])\n",
    "\n",
    "    vocab = d2l.Vocab(train_tokens, min_freq=5, reserved_tokens=['<pad>'])\n",
    "\n",
    "    train_features = torch.tensor([d2l.truncate_pad(vocab[line], num_steps, vocab['<pad>']) for line in train_tokens])\n",
    "    test_features = torch.tensor([d2l.truncate_pad(vocab[line], num_steps, vocab['<pad>']) for line in test_tokens])\n",
    "    \n",
    "    train_labels = torch.tensor(train_data[1], dtype=torch.float32)\n",
    "    test_labels = torch.tensor(test_data[1], dtype=torch.float32)\n",
    "\n",
    "    train_dataset = TensorDataset(train_features, train_labels)\n",
    "    test_dataset = TensorDataset(test_features, test_labels)\n",
    "\n",
    "    train_iter = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_iter = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    ### END OF YOUR CODE\n",
    "    return train_iter, test_iter, vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "model_name = 'roberta-base' # Can change to other models of roberta\n",
    "max_len = 512 \n",
    "batch_size = 4\n",
    "epochs = 3\n",
    "learning_rate = 1e-6\n",
    "eps = 1e-8\n",
    "\n",
    "train_iter, test_iter, vocab = load_data_imdb(batch_size)\n",
    "\n",
    "test_accuracies = []\n",
    "\n",
    "  # Load RoBERTa model with classification head (2 classes for sentiment)\n",
    "model = RobertaForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "model.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate, eps=eps)\n",
    "\n",
    "criterion = CrossEntropyLoss()\n",
    "\n",
    "print(\"Starting Training\")\n",
    "\n",
    "for epochs in range(0, epochs):\n",
    "  model.train()\n",
    "  total_loss = 0.0\n",
    "  total_num = len(train_iter)\n",
    "  correct = 0\n",
    "  for features, labels in train_iter:\n",
    "      optimizer.zero_grad()\n",
    "      # features, labels = features.to(device), labels.to(device)\n",
    "\n",
    "      # out = model(features)\n",
    "      # loss = criterion(out, labels.long())\n",
    "      input_ids = features.to(device)\n",
    "      attention_mask = (features != vocab['<pad>']).to(device)\n",
    "      labels = labels.to(device, dtype=torch.long)\n",
    "\n",
    "      # Forward pass\n",
    "      outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "      loss = outputs.loss\n",
    "      logits = outputs.logits\n",
    "\n",
    "      total_loss += loss.item()\n",
    "\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "\n",
    "      _, predicted = torch.max(logits, 1)\n",
    "      total_num += labels.size(0)\n",
    "      correct += (predicted == labels).sum().item()\n",
    "    \n",
    "  train_acc = 100 * correct/total_num\n",
    "  #train_accuracies.append(train_acc)\n",
    "  #train_losses.append(total_loss)\n",
    "  print(f\"Epoch {epochs}; Training accuracy: {train_acc:.2f}%\")\n",
    "\n",
    "  model.eval()  \n",
    "  correct = 0\n",
    "  total_num = 0\n",
    "\n",
    "  predictions, true_labels = [], []\n",
    "\n",
    "  with torch.no_grad():  \n",
    "      for features, labels in test_iter:\n",
    "      # Move data to device\n",
    "          input_ids = features.to(device)\n",
    "          attention_mask = (features != vocab['<pad>']).to(device)\n",
    "          labels = labels.to(device)\n",
    "\n",
    "          # Forward pass\n",
    "          outputs = model(input_ids, attention_mask=attention_mask)\n",
    "          logits = outputs.logits\n",
    "          predictions.extend(torch.argmax(logits, dim=-1).cpu().numpy())\n",
    "          true_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "  accuracy = accuracy_score(true_labels, predictions)\n",
    "  print(f\"Test Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
